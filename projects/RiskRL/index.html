<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <meta name="description" content="When to Localize? A Risk-Constrained Reinforcement Learning Approach">
  <meta name="keywords" content="Reinforcement Learning, Risk-Constrained RL, Active Localization, Robotics">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <title>When to Localize? A Risk-Constrained Reinforcement Learning Approach</title>
  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro" rel="stylesheet">
  <link rel="stylesheet" href="./static/css/bulma.min.css">
  <link rel="stylesheet" href="./static/css/index.css">
  <link rel="stylesheet" href="./static/css/fontawesome.all.min.css">
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <script defer src="./static/js/fontawesome.all.min.js"></script>
</head>
<body>

<nav class="navbar" role="navigation" aria-label="main navigation">
  <div class="navbar-brand">
    <a role="button" class="navbar-burger" aria-label="menu" aria-expanded="false">
      <span aria-hidden="true"></span>
      <span aria-hidden="true"></span>
      <span aria-hidden="true"></span>
    </a>
  </div>
  <div class="navbar-menu">
    <div class="navbar-start" style="flex-grow: 1; justify-content: center;">
      <a class="navbar-item" href="http://raaslab.org/">
        <span class="icon">
            <i class="fas fa-home"></i>
        </span>
      </a>
      <div class="navbar-item has-dropdown is-hoverable">
        <a class="navbar-link">Table of Contents</a>
        <div class="navbar-dropdown">
          
          <a class="navbar-item" href="#motivation">Abstract</a>
          <a class="navbar-item" href="#framework">Overview</a>
          <a class="navbar-item" href="#results">Results</a>
        </div>
      </div>
    </div>
  </div>
</nav>

<section class="hero">
  <div class="hero-body">
    <div class="container is-max-desktop">
      <h1 class="title is-1">When to Localize? A Risk-Constrained Reinforcement Learning Approach</h1>
      <h2 class="subtitle is-4">
        <a href="mailto:cshek1@umd.edu">Chak Lam Shek*</a>,
        <a href="https://ktorsh.github.io/">Kasra Torshizi*</a>,
        <a href="https://troiwill.github.io/">Troi Williams</a>,
        <a href="http://tokekar.com">Pratap Tokekar</a>
      </h2>
      <h3 class="subtitle is-5">University of Maryland, College Park</h3>
      <div style="text-align: center; margin-top: 1rem;">
        <a href="https://arxiv.org/abs/2411.02788" class="external-link button is-normal is-rounded is-dark">
          <span class="icon"><i class="ai ai-arxiv"></i></span>
          <span>arXiv</span>
        </a>
      </div>
    </div>
  </div>
</section>

<section class="section">
  <div class="gif-row">
    <div class="gif-col">
      <img src="static/images/m2l.gif" class="gif" alt="M2L">
      <p>M2xL (Move Twice, Localize Once)</p>
    </div>
    <div class="gif-col">
      <img src="static/images/base.gif" class="gif" alt="BaseRL">
      <p>BaseRL</p>
    </div>
    <div class="gif-col">
      <img src="static/images/risk.gif" class="gif" alt="RiskRL">
      <p>RiskRL</p>
    </div>
  </div>
  <br>
  <div class="container is-max-desktop">
    <p>Above are simulations of our contribution, RiskRL, along with two baselines, M2xL and BaseRL. M2xL policy while leading to high success rates, uses an execessive amount of localizations. BaseRL, while 
      greatly reducing the number of localizations, puts the agent into risky situations. On the otherhand, RiskRL is able to effectly balance these two principals to keep a high success rate while also 
    keeping the number of localizations low. </p>
  </div>  
</section>

<section class="section">
  <div class="container is-max-desktop">
    <h2 class="title is-3" id="motivation">Abstract</h2>
    <div style="text-align: center;">
      <img src="static/images/when-to-localize-idea.jpg" alt="Motivation" style="width:60%; margin: 0 auto;">
    </div>
    <p style="margin-top: 10px;">Consider a robot that may want to seldom localize (e.g., due to resource constraints) while traveling along a path. Despite obstacles, it can execute a series of open-loop motions. However, dead reckoning uncertainty grows, increasing failure risk, so the robot must localize eventually. The idea is applicable to general search-and-rescue scenarios like a submersible searching for a black box underwater.</p>
  </div>
</section>

<section class="section">
  <div class="container is-max-desktop">
    <h2 class="title is-3" id="framework">Overview</h2>
    <section class="section">
      <div class="container is-max-desktop">
        <div style="text-align: center;">
          <img src="static/images/Flow_Chart2.png" alt="Flowchart" style="width:80%; margin: 20px auto;">
        </div>
        <div style="text-align: center;">
          <img src="static/images/rl_network.png" alt="RL Network" style="width:80%; margin: 20px auto;">
        </div>
        <p>
          This flowchart illustrates our algorithmic framework for solving the active localization problem. The system integrates a high-level planner based on Soft Actor-Critic (SAC) reinforcement learning for decision-making, a low-level planner for executing actions and triggering localization or replanning, and a Particle Filter (PF) for belief estimation. The PF updates the agent's belief using actions and noisy pose observations, and generates an observation vector used by the high-level planner to guide decision-making.
        </p>
        <br>
    
        <!-- <p>
          <strong>Chance-Constrained Planning.</strong> The original optimization objective does not align with standard RL. We reformulate it to maximize:
        </p>
        <div style="text-align: center;">
          <img src="static/images/Equation1.png" alt="Equation 1" style="width:50%; margin: 10px auto;">
        </div>
        <p style="text-align: left;">
          while ensuring:
        </p>
        <div style="text-align: center;">
          <img src="static/images/Equation2.png" alt="Equation 2" style="width:60%; margin: 10px auto;">
        </div>
        <p style="text-align: left;">
          The new reward function becomes:
        </p>
        <div style="text-align: center;">
          <img src="static/images/Equation3.png" alt="Equation 3" style="width:60%; margin: 10px auto;">
        </div>
        <p style="text-align: left;">
          This formulation allows us to optimize with standard SAC using a primal-dual method.
        </p> -->
        
      </div>
    </section>

    <!-- <p>
      We adopt a Soft Actor-Critic model equipped with an LSTM to handle noisy or incomplete observations. The LSTM enables the agent to learn temporal dependencies from the history, leading to more stable and generalized decision-making in uncertain environments.
    </p> -->
  </div>
</section><section class="section">
  <div class="container is-max-desktop">
    <h2 class="title is-3" id="results">Results</h2>

    <p style="margin-top: 20px;">
      We evaluate our method across different environments and noise conditions. Each subsection below addresses a core research question to systematically evaluate the capabilities of our RiskRL algorithm.
    </p>

    <div style="text-align: center;">
      <h3 class="title is-4" style="margin-top: 30px;">How does the robot behave in a single noisy and risky environment?</h3>
      <img src="static/images/QE.png" alt="Qualitative example of robot navigation: actual path (blue), estimated path and localization positions (red)." style="width:50%; margin-bottom: 30px;">
      <p style="text-align: left; margin-top: -10px;">
        The figure demonstrates robot adaptation in a noisy environment (transition noise: 80%, 10%, 10%; observation noise: 68%, 4%; risk threshold: 40%). The robot frequently localizes in high-risk areas like the Start Area and Middle Tunnel, reducing localization in safer regions, demonstrating effective balance between safety and efficiency.
      </p>

      <h3 class="title is-4" style="margin-top: 30px;">How does RiskRL compare to other planners in overall performance?</h3>
      <div class="columns is-centered">
        <div class="column is-6">
          <img src="static/images/num_localize_bar.png" alt="Number of localizations across environments." style="width:70%;">
        </div>
        <div class="column is-6">
          <img src="static/images/success_rate_bar.png" alt="Success rates across environments." style="width:70%;">
        </div>
      </div>
      <p style="text-align: left;">
        RiskRL significantly outperforms other algorithms in challenging environments (chesapeake, st_thomas) with higher success rates and efficient localization counts. BASE performs competitively with fewer localizations, whereas CC-POMCP, though effective, is computationally slower (20 seconds/action vs. 20 milliseconds for RiskRL).
      </p>

      <h3 class="title is-4" style="margin-top: 30px;">How does RiskRL adapt under different noise and risk settings?</h3>
      <div class="columns is-multiline is-centered">
        <div class="column is-4"><img src="static/images/tran_loc.png" alt="Localization counts at different transition noise levels." style="width:100%;"></div>
        <div class="column is-4"><img src="static/images/obs_num.png" alt="Localization counts under different observation noises." style="width:100%;"></div>
        <div class="column is-4"><img src="static/images/risk_num.png" alt="Localization counts under different risk thresholds." style="width:100%;"></div>
        <div class="column is-4"><img src="static/images/tran_win.png" alt="Success rates at different transition noise levels." style="width:100%;"></div>
        <div class="column is-4"><img src="static/images/obs_win.png" alt="Success rates under different observation noises." style="width:100%;"></div>
        <div class="column is-4"><img src="static/images/risk_win.png" alt="Success rates under different risk thresholds." style="width:100%;"></div>
      </div>
      <p style="text-align: left; margin-top: 20px;">
        RiskRL adapts localization frequency to increased transition noise while maintaining stable success rates. Observation noise minimally impacts performance, demonstrating robustness to sensory uncertainty. Adjusting risk constraints highlights a clear trade-off: stricter constraints increase localization but reduce failures; lenient constraints reduce localization at the expense of lower success rates.
      </p>

      <h3 class="title is-4" style="margin-top: 30px;">Where does the robot localize most frequently across the environment?</h3>
      <img src="static/images/heatmap.png" alt="Probability heatmap of localization across environments." style="width:80%; margin-top: 30px;">
      <p style="text-align: left; margin-top: 10px;">
        Heatmaps illustrate localization probability relative to estimated locations. Yellow denotes start positions; green denotes goals. Chesapeake and St. Thomas environments show representative segments. Algorithms were tested 250 times per predefined path.
      </p>
    </div>
  </div>
</section>

<footer class="footer">
  <div class="container">
    <div class="content has-text-centered">
      <pre style="text-align: left; margin-top: 20px;">
@misc{shek2024localize,
  title={When to Localize? A Risk-Constrained Reinforcement Learning Approach},
  author={Shek, Chak Lam and Torshizi, Kasra and Williams, Troi and Tokekar, Pratap},
  year={2024},
  eprint={2403.12345},
  archivePrefix={arXiv},
  primaryClass={cs.RO}
}</pre>
      <p>This website is licensed under CC BY-SA 4.0.</p>
    </div>
  </div>
</footer>

</body>
</html>